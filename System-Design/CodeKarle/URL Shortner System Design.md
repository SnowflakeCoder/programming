# URL Shortner System Design

Design a URL shortening service like **tinyurl.com** / **Bitly**. 

## Functional Requirements

- Given a long URL you return a short URL.
- When somebody hits a short URL then you basically redirect to the longer URL.

## Non-functional Requirements 

- Service needs to be **highly available**.
- It should work at a very **low latency**.

## Short URL Length

The very first thing to answer is what should be the length of the short URL. That depends upon the **scale that we need to build** this system for because we **should not run out of the short URL**. Another option is we build a system agnostic of the length and we start from somewhere and it keeps on incrementing. But its better to start with a fixed length. So the very first question you need to ask is **what is the traffic** you are looking at? **how many unique URLs** will come for shortening probably every day / every month and until **what duration you need to support** that.

Let's just assume that, once a URL is converted into a shorter URL you need to save it for at least next ten years. Also let's just say there are X number of requests that comes to you every second, so per year it will be **X\*60\*60\*24\*365**. So you should be able to build a system which can handle these number of unique requests.

## CharacterSet & Length

Next question we need to answer is What are characters can you include in the short URL so you should come up with a **character set that is allowed for short URLs**. Generally people take the capital A-Z and small a-z and numbers. So these are 62 alphabets which you can use in the shortURLs.

Now you need to come up with the length which can handle these number of unique URLs (unique combinations of these 62 characters).  So basically you need to come up with a number where 

pow(62,n) > **X\*60\*60\*24\*365\*10**  => to support for 10 years.





why okay you solve this equation and you get the answer to n okay so n would be log base 62 of life okay the feeling of it let's say if this number comes down to me four point five then you take n is equal to five so you will create shorty or s of length side okay now as a general number 62 to the power 6 is somewhere around 58 region if I am NOT wrong and 60 2 to the power 7 is somewhere around three point five trillion okay now both of these are very massive numbers depending upon what is your use case you can come up with some length let's just say for our use if you stick to seven characters generally I have seen it's a standard that the short URL still generally used seven characters may be more at times but will stick to seven characters and we will be able to support three point five trillion unique URLs okay that I am assuming is more than this numbers if it so happens is that this by is even more then you can probably take eight characters main characters or whatever mean now let's look at one of the potential architectures that can solve this problem now this is not a final version will iterate over it so let's just say there is this UI which takes the long URL as input and gives back the short URL so it will call the short URL service there'll be multiple instances of the service which somehow generate a short URL and stores it in the database and returns the short URL to the customer on the other side this could also be used by when somebody hits a short URL where in the shorter service boot space is a longer is from the database and then redirect the user to the longer URL okay this looks nice but then we have not covered one important point that how does this service really generate a short wearer okay so even though we had 262 characters for all the communication further we'll just assume you just use numbers so that it's easy to calculate and think over it now let's just say there are multiple instances of this service okay and all of them are getting some requests all right this service could also generate a number one one one this service could also generate a number one one one and then we will end up with a collision right if two services generate a same short URL or two requests then we have a problem now in technical terms it's called a collision but for us it's a problem because we cannot have one short URL pointing to two long URLs now you can say that the possible solution could be that we first check in the database and then you know kind of retry that would be fine but then the problem is that's not really efficient okay so what we need is a predictable way to generate a short URL knowing that there would be no collisions at all notice now one very simple way to do that is using the one of the features of rediff okay so let's just say we use a Redis cluster okay and all of these services request a number from Redis Redis make sure that it will always return a unique number so basically what it will do is it will start the counting from 1 all the way up to billions trillions whatever right and each time it gets a request it increments the counter in response backs good so now we will make sure that all of these guys are getting unique numbers and some unique number at base 10 they can convert to base 62 and generate the short one okay now that would work fine but then we have a problem the problem is that first of all everybody now is connecting to Redis okay so we have ready under a huge amount of load second problem is remember one of the important key things of any design interview you should never create a single point of failure now what is happening is in all the requests that have to generate a short URL this Redis becomes a single wonderfulness and there is no backup to it right if this goes down then there is no we can't recover right which is a problem now you can argue that we can have multiple Rattus but that's also tricky moreover if the scale becomes beyond the capacity of one Redis machine then we are in much bigger problem why because let's say if Bunder this is not able to scale to the latency requirements that we wanted to this will again start choking the whole system now another alternate is that we keep multiple registrate that will give us high availability and it will give us better performance because now there are multiple machines and some of these machines will talk to this Redis and let's say this machine talk to the straightest right this would work just fine till the time these Redis don't start generating duplicate numbers if both the rightest are starting from the same index again they'll start generating duplicates so what we need to do is somehow make sure that both these readies do not generator same numbers right one very simple way is to give a series to this Redis another series to this Redis to standards that would work fine but what if you have to add a third Redis now it becomes complicated now you need somebody to manage what series are being given to whom right plus once you have that management piece we might as well try to get away from rated ok so now let's try to look at a slightly more advanced system which can generate unique URLs without using a Redis without having a single point of PDF now let's look at one of the possible solutions which have solved this problem remember what we need whenever we get a long URL and we need to convert it to a short URL we basically need to make sure that our service machine should be able to generate a unique number which will never ever get repeated even by any other instance of the service so each of these two service instances to generate a number which other ones will never ever generate convert that numbers on-base tend to basics t2 and return back right this is the flow that we need what we've done is again going over the UI so this is the green thing is basically the long to short way from where somebody can give a long year and expect to get a shot or 11 okay okay it talks to a load balancer a month behind it are multiple instances of short URL service what I am trying to now do is I have introduced a token service the idea is all of these guys need to have a number right and making sure that none of them none of the other machines generate the same number one of the simple ways to do that is assigned ranges to each of the machines right I can say that this tokens so the shorter a service will request a token range from token service right each time token service gets a request it will turn on a single threaded model and it will give a range to any of the service instances okay it's let's say it says that I am giving a range 1 to 1,000 to this machine ok now this machine on startup will also call token service so all of these services called token service either when they start up or when they are running out of the range okay now let's hit this service water range from 1001 - mm okay similarly other instances of the same service would also get some rings now because this works and this can be this token service can be built on top of my sequence because it runs at a very low skill it it will get a call once in a couple of hours energy return like the ranges right but now I have taken these numbers as one two thousand realistically it will be a bit happier number so that at least it can run for an hour or so okay so now token service has given numbers to each of these services now the flow is very simple let's say a request reaches this service it takes the first number 1001 converts that into waste if you do and return the output ok obviously save after saving it the next request comes it takes it increment it takes 1 0 0 2 fills it in Cassandra returns back next Express request comes in it uses 1 0 0 3 now let's say it somehow gets lot of requests gets to 2000 at that point in time maybe a bit before that also when we are closing the end range it is query token service and get a next stage so let you know it the next thing that the service got is five thousand one to six thousand six way so now it will continue processing listen now when 5000 to 6000 is assigned to this service when this service will make a request it can possibly get six thousand one to seven thousand but it will never get this five thousand one to six thousand H why because last thing is maintained by token service and the way it would probably work is it will keep a range as a record and keep a flag whether it is assigned or not okay in a simple my sequel on time matching basis you get a record when you get a request you take the first unassigned token range and return that okay and because it will then sit on top of my sequel database it will make sure that it is always unique would so this becomes kind of our read flow now let's say there is a massive amount of traffic okay all we need to do is keep multiple instances of token service at max even though it might so there are multiple ways to scale it first of all we can increase the length of the range so that it doesn't get bombarded too often so instead of thousand dollars we can allocate millions of tokens at once so it doesn't get too many requests and anyway this service will be distributed in multiple geographies at least and multiple data centers so that this doesn't become a single word affiliate good now there are some problems in this problem possible problems are what if this service got this 5000 to 6000 range started working for a while used a couple of token and then it kind of got shut down and died right let's say there was an out of memory error and this process got killed what happens to those tokens right so those tokens go of it go away as in they there is no track record of how many tokens are being used all what is happening is it is iterating over that list in memory and assigning one token to each request right if the service dies down it will probably on the same machine it will get started up again and the next time we will get another token range right so let's say third time it gets a token range of nine thousand one to ten thousand right now also a noose tokens of this 5000 to 6000 range there is no record of them so they are kind of gone forever now if you look at it let's say even if this happens a couple of times in a day right overall how many tokens can be if with the length of seven for a short URL we are able to support three point five trillion unique numbers right that is a very massive numbers now if you lose out a couple of thousands of tokens out of this range it is like taking a bucket out of an ocean right it doesn't really matter so even if you lose out some tokens it's okay but if you start tracking those tokens then it will become a fairly complicated system so just to make it easy for us to develop we will say that okay when the system shuts down all those tokens go away there is no we don't need them we will get a new range of token and work out of that so that is one thing that we could do so this is basically the long URL to short URL path now on the other side when a short URL is hit all you do is you get a request in short URL you hit your database and you get the longer URL you send it back to the service the service does a redirect and the user gets redirected to the manual okay now one thing I've not answered is why did I user Cassandra ideally I could have used any database all we need to do is to keep a database which can handle these many number of unique URLs okay now a my sequel at these number of Records would start giving some problems you can shut it probably and make it work and I will for sure work so that's the reason I've kept the sender you can try using a my sequel here with enough sharding and all of that that would possibly also work fine so that's not that big of a problem does this token service is also using a Mexican so that would be shared across both the solution is all right so whatever we have built till now is good enough from a functional and non-functional standpoint and it does what we were supposed to build but is it good enough definitely not well because this system what we have build does not give us any metrics about how it is being used what kind of urls are the most fields URL what kind of geographies people come from and it does not give any enough metal that we would want to have out of the system so for example whenever a person is generating a short URL wouldn't it be nice if we can tell them what kind of geography to your people come from or what kind of hits are you getting or what kind of user agents or devices the people are connecting from all of those things would be valuable information for any person who's generating a short URL similarly for us let's say if we have multiple data centers let's say for example we have four data centers and we want to choose any two of them as primary and two of them a stand by data centers - and their spread of now if you have to decide which ones should be primary and which one should be secondary normally what companies do is they just make a decision based on somebody's gut feeling but we could make a data-driven this decision over here depending upon what kind of traffic we are getting from what kind of geography and wherever we are getting most amount of traffic from the data centers closer to those geographies could be made as primary data center so for doing all of these things it will be good to have a good enough amount of analytics that the system emits out so let's look at how can we do that so now let's see how the analytics is being good so the normal flow the way it works is whenever we get a request with a short URL let's say a client sends us a request right week when your database get the longer URL from the database based on the short URL and return it back to the client which then redirects the user to the longer URL right instead of just simply returning the URL what we essentially do is each time the request comes to us that it will come with a lot of attributes it will give us some origin header saying what can what is the platform from where this request has come in feel excessive posted this link on a Facebook or a LinkedIn kind of a platform it will have that information it will also have information about the user agent which is calling let's say if it's an Android application iOS application or a browser it'll have that kind of an information as well and it will also have a source IP address right all of these information before sending out the response we will also put that into a cop car which will be used to then power the analysis okay now based on the IP address we'll also be able to come up with what country it is so all of those things can be taken care of on the analysis side okay but if we start putting into Kastner on each request when we get the request it will impact our non-functional requirement of latency why because now we are doing an additional a step in the process and that length is the latency so I would not recommend doing that what instead we could do as a fairly nice solution is make that Casper request as a parallel call so you can have a different thread in which you can send the Kafka right and it is returned back to the user and asynchronously it can be returned to copper now what is the problem in doing an asynchronous thing there potential possibility that the Casca right could sell for whatever reason and you have returned back to the user so you'll miss certain analytics right now in this kind of a system because payment and all is not involved it's just some simple analytics we should be okay if we are losing out on certain events I think it's okay if we build it this way also worst case we'll just lose out on some analytics which is fine right but can we improve it even further so the problem with this approach is each time we write to Kafka it involves some IO right there's the CPU involved there's a I you and well there's a network traffic and there's a network transfer in what way can we avoid all of this doing all of this on each call probably so what we could do is instead of writing into cache on each request we could maybe aggregate that information locally in a machine so maybe you can have a queue or some kind of a data structure in which you are persisting each record that we got a request for this short URL with count 1 the request again comes in you increment the count to count or something of that sort you could implement it or you could simply make it as a queue of all the requests now whenever the size of the data structure process some threshold or you could do it a time base and also laying every 10 seconds will flush that so in whatever condition you can flush the data into that data factor into one column Kafka so basically you are reducing the i/o that you are doing the dish request and doing it as a batch right the benefit you get is you can have now drive much more performance out of that single machine so thereby helping us in the non-functional requirements of high availability low latency all of that and secondly it will improve the overall performance of the system again the drawback here is now you can lose out not just one event but a lot of events now based on your conversation with your interview and decide on is that ok or not personally I would say it's a fairly ok thing if you lose out on 10 20 30 events that's perfectly fine cool so now we have put all the events into kaftan now what happens so there are a bunch of ways you can implement analytics one very simple approaches you can dump all of this data and to huddle and then build some hive queries on some kind of queries on top of it which will generate you aggregate results right alternatively you could also have a start steaming job running which comes up every 10 minutes let's say and takes all the returned in last 10 minutes and does some aggregate analysis on top of it saying which URL was it how many times with geography people came in how many times something of that sort and dumps the aggregate information into a datastore which can then be used to power various kinds of analytics that user can see so you could implement it in either ways you want based on the conversation with your interview so yeah I think this should be it for a URL shortening system thanks for watching this video if you have any suggestions on what videos we should make next or how we could improve this one do let us know by commenting here and don't forget to subscribe to this channel like the videos and share the videos with your friends while we keep working to get more such content to you happy learning

