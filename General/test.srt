
1
00:00:00,000 --> 00:00:01,920
how to choose the best garbage collector

2
00:00:01,920 --> 00:00:04,589
for your use case before we do that

3
00:00:04,589 --> 00:00:06,150
let's try to understand some of the

4
00:00:06,150 --> 00:00:08,910
garbage collection concepts we will talk

5
00:00:08,910 --> 00:00:11,160
about trade-offs their generational GC

6
00:00:11,160 --> 00:00:13,230
and two algorithms called mark and copy

7
00:00:13,230 --> 00:00:15,719
and mark and sweep so let's start with

8
00:00:15,719 --> 00:00:18,150
trade-offs when we talk about garbage

9
00:00:18,150 --> 00:00:19,800
collection we generally have to consider

10
00:00:19,800 --> 00:00:22,199
three things the first thing is the

11
00:00:22,199 --> 00:00:24,570
memory this is the amount of memory that

12
00:00:24,570 --> 00:00:26,820
is assigned to the program this is also

13
00:00:26,820 --> 00:00:29,609
called as the heap the second thing that

14
00:00:29,609 --> 00:00:32,238
we need to understand is the true put

15
00:00:32,238 --> 00:00:35,520
throughput is how much amount of time

16
00:00:35,520 --> 00:00:38,579
that a code is run compared to how much

17
00:00:38,579 --> 00:00:40,680
amount of time your garbage collection

18
00:00:40,680 --> 00:00:42,930
is run so for example if the throughput

19
00:00:42,930 --> 00:00:47,070
is 99% that means that 99% of the time

20
00:00:47,070 --> 00:00:49,469
your code was running and 1% of the time

21
00:00:49,469 --> 00:00:52,170
that garbage collection was running so

22
00:00:52,170 --> 00:00:54,420
of course ideally you would want as much

23
00:00:54,420 --> 00:00:57,480
higher throughput as possible and the

24
00:00:57,480 --> 00:00:59,280
third aspect that we need to understand

25
00:00:59,280 --> 00:01:02,250
is the latency latency is whenever

26
00:01:02,250 --> 00:01:04,799
garbage collection runs how much amount

27
00:01:04,799 --> 00:01:07,140
of time our program stops for the

28
00:01:07,140 --> 00:01:09,720
garbage collection to run properly these

29
00:01:09,720 --> 00:01:11,700
are generally measured in milliseconds

30
00:01:11,700 --> 00:01:14,189
but they can go up to a few seconds

31
00:01:14,189 --> 00:01:16,380
depending on the size of the memory and

32
00:01:16,380 --> 00:01:18,509
the garbage collection algorithm that

33
00:01:18,509 --> 00:01:21,600
issues ok of course ideally we would

34
00:01:21,600 --> 00:01:23,369
want the latency to be as low as

35
00:01:23,369 --> 00:01:26,970
possible or as predictable as possible

36
00:01:26,970 --> 00:01:29,610
the second concept that we need to learn

37
00:01:29,610 --> 00:01:32,490
is called generational hypothesis this

38
00:01:32,490 --> 00:01:35,490
hypothesis says that most objects die

39
00:01:35,490 --> 00:01:35,970
young

40
00:01:35,970 --> 00:01:38,400
so whenever you use any object or

41
00:01:38,400 --> 00:01:40,950
variable whenever you create object or

42
00:01:40,950 --> 00:01:43,619
variable within a function or within a

43
00:01:43,619 --> 00:01:46,229
for loop and that does not escape the

44
00:01:46,229 --> 00:01:48,600
for loop or function so it is limited

45
00:01:48,600 --> 00:01:51,240
only to that scope then of course as

46
00:01:51,240 --> 00:01:53,009
soon as the core comes out of the for

47
00:01:53,009 --> 00:01:55,009
loop or comes out of the function that

48
00:01:55,009 --> 00:01:57,390
object is eligible for garbage

49
00:01:57,390 --> 00:02:00,479
collection so this hypothesis says that

50
00:02:00,479 --> 00:02:02,369
most of our variables are of this kind

51
00:02:02,369 --> 00:02:05,759
they die young and that is why many of

52
00:02:05,759 --> 00:02:08,038
the garbage collection algorithms split

53
00:02:08,038 --> 00:02:10,288
your heap size into what is called a

54
00:02:10,288 --> 00:02:12,890
young generation and an origin

55
00:02:12,890 --> 00:02:15,920
so whenever you first create the objects

56
00:02:15,920 --> 00:02:17,330
they are kept in to the young generation

57
00:02:17,330 --> 00:02:20,420
of course most of them die young or they

58
00:02:20,420 --> 00:02:22,160
are eligible for garbage collection very

59
00:02:22,160 --> 00:02:24,380
quickly that's why you have a lot of

60
00:02:24,380 --> 00:02:26,510
garbage collection run only on this

61
00:02:26,510 --> 00:02:28,940
young generation and that collection is

62
00:02:28,940 --> 00:02:32,660
called a minor collection if there are

63
00:02:32,660 --> 00:02:34,430
objects like your class level variables

64
00:02:34,430 --> 00:02:36,140
which are also called instance level

65
00:02:36,140 --> 00:02:39,470
variables then they of course remain

66
00:02:39,470 --> 00:02:42,530
that any or or the lifetime remains much

67
00:02:42,530 --> 00:02:45,319
longer and that is why even after a lot

68
00:02:45,319 --> 00:02:47,239
of minor collections when the objects

69
00:02:47,239 --> 00:02:49,700
are still not eligible for garbage

70
00:02:49,700 --> 00:02:52,010
collection they are promoted into this

71
00:02:52,010 --> 00:02:54,620
old generation whenever there are a lot

72
00:02:54,620 --> 00:02:56,360
of objects in the old generation so

73
00:02:56,360 --> 00:02:59,750
let's say if the amount of space

74
00:02:59,750 --> 00:03:01,880
occupied in the old generation goes

75
00:03:01,880 --> 00:03:05,360
beyond the threshold for example 60% or

76
00:03:05,360 --> 00:03:07,940
70% then there is a major collection

77
00:03:07,940 --> 00:03:09,769
triggered because now you have to clean

78
00:03:09,769 --> 00:03:12,920
up the old generation also and in this

79
00:03:12,920 --> 00:03:14,599
case generally a different kind of

80
00:03:14,599 --> 00:03:17,680
algorithm is run on the older generation

81
00:03:17,680 --> 00:03:19,780
now the third concept that we need to

82
00:03:19,780 --> 00:03:21,430
understand is the mark and copy

83
00:03:21,430 --> 00:03:24,310
algorithm so within the young generation

84
00:03:24,310 --> 00:03:27,040
generally the space is divided as hidden

85
00:03:27,040 --> 00:03:30,430
space and to survivor spaces so all the

86
00:03:30,430 --> 00:03:32,860
new objects are allocated to the hidden

87
00:03:32,860 --> 00:03:36,189
space whenever time comes for a mine

88
00:03:36,189 --> 00:03:39,340
collection only the live objects of this

89
00:03:39,340 --> 00:03:42,790
leaden space is copied over to the cyber

90
00:03:42,790 --> 00:03:46,000
survivor space and this involves both

91
00:03:46,000 --> 00:03:48,220
things so it will first mark all the

92
00:03:48,220 --> 00:03:50,290
objects which are live that means which

93
00:03:50,290 --> 00:03:52,030
are still being used and not eligible

94
00:03:52,030 --> 00:03:54,459
for garbage collection and in second

95
00:03:54,459 --> 00:03:56,319
stage it will copy over all those live

96
00:03:56,319 --> 00:03:58,510
objects in this survivor space either

97
00:03:58,510 --> 00:04:01,840
one or two once it copies all the live

98
00:04:01,840 --> 00:04:04,420
objects now this Eden space consists of

99
00:04:04,420 --> 00:04:05,950
objects which are you have already

100
00:04:05,950 --> 00:04:07,510
copied and the objects which are

101
00:04:07,510 --> 00:04:09,700
eligible for garbage collection and that

102
00:04:09,700 --> 00:04:11,829
is why this whole Eden space is wiped

103
00:04:11,829 --> 00:04:15,129
out or it is just considered that it is

104
00:04:15,129 --> 00:04:16,720
an empty space there is nothing of

105
00:04:16,720 --> 00:04:19,329
elements there and the allocation of new

106
00:04:19,329 --> 00:04:22,539
objects starts again and you can take a

107
00:04:22,539 --> 00:04:23,949
look at these steps to understand why

108
00:04:23,949 --> 00:04:25,870
there are two spaces survivor space one

109
00:04:25,870 --> 00:04:28,510
and two the second algorithm and the

110
00:04:28,510 --> 00:04:30,220
fourth concept that we need to

111
00:04:30,220 --> 00:04:32,740
understand is the algorithm called

112
00:04:32,740 --> 00:04:36,010
mark-sweep and compact algorithm this is

113
00:04:36,010 --> 00:04:37,650
generally done on the old generation

114
00:04:37,650 --> 00:04:39,970
here let's say we have a lot of

115
00:04:39,970 --> 00:04:42,130
allocated objects some of them on life

116
00:04:42,130 --> 00:04:44,380
some of them are eligible for garbage

117
00:04:44,380 --> 00:04:47,199
collection first will mark only the live

118
00:04:47,199 --> 00:04:49,240
objects right so we'll mark this object

119
00:04:49,240 --> 00:04:51,010
which is live which is this object and

120
00:04:51,010 --> 00:04:54,070
this object second is we will sweep and

121
00:04:54,070 --> 00:04:56,260
we'll remove all the objects which are

122
00:04:56,260 --> 00:04:58,000
eligible for garbage collection so we'll

123
00:04:58,000 --> 00:05:01,780
remove all the spaces here at the top

124
00:05:01,780 --> 00:05:04,800
and make it black generally of course

125
00:05:04,800 --> 00:05:07,000
technically speaking we do not remove it

126
00:05:07,000 --> 00:05:09,099
we just update a data structure behind

127
00:05:09,099 --> 00:05:10,659
the scene saying that these spaces are

128
00:05:10,659 --> 00:05:13,810
now empty and then the third aspect of

129
00:05:13,810 --> 00:05:16,300
this is compaction so we'll move all

130
00:05:16,300 --> 00:05:18,190
these live objects which are still being

131
00:05:18,190 --> 00:05:20,349
used and we'll move them on to the left

132
00:05:20,349 --> 00:05:23,139
side and we'll fluster them together the

133
00:05:23,139 --> 00:05:24,729
advantage of this compaction is

134
00:05:24,729 --> 00:05:27,159
afterwards when you when you want to

135
00:05:27,159 --> 00:05:29,680
allocate new objects all you have to do

136
00:05:29,680 --> 00:05:31,090
is keep a pointer here

137
00:05:31,090 --> 00:05:33,550
right keep a reference that everything

138
00:05:33,550 --> 00:05:35,560
on the left is being utilized everything

139
00:05:35,560 --> 00:05:37,960
on the right is free and whenever you

140
00:05:37,960 --> 00:05:39,460
want to allocate a new object just put

141
00:05:39,460 --> 00:05:42,850
that object here and bump the pointer to

142
00:05:42,850 --> 00:05:44,530
the end of that object so again

143
00:05:44,530 --> 00:05:46,480
everything on the left side is being

144
00:05:46,480 --> 00:05:47,950
utilized and everything on the right

145
00:05:47,950 --> 00:05:51,430
side is free now that we understand all

146
00:05:51,430 --> 00:05:54,130
these four concepts let's take a look at

147
00:05:54,130 --> 00:05:56,250
the actual garbage collection algorithms

148
00:05:56,250 --> 00:05:58,480
serial collector has the smallest

149
00:05:58,480 --> 00:06:00,760
footprint of any of the collectors so

150
00:06:00,760 --> 00:06:02,680
then the amount of data structures the

151
00:06:02,680 --> 00:06:04,479
footprint required for this garbage

152
00:06:04,479 --> 00:06:06,810
collector to run is very very minimal

153
00:06:06,810 --> 00:06:09,490
this collector uses single thread for

154
00:06:09,490 --> 00:06:11,200
both minor as well as miniature

155
00:06:11,200 --> 00:06:13,840
collections and as we saw in the earlier

156
00:06:13,840 --> 00:06:17,050
slide about compactions and bump the

157
00:06:17,050 --> 00:06:19,150
pointer technique the serial collector

158
00:06:19,150 --> 00:06:20,680
uses that technique to allocate any

159
00:06:20,680 --> 00:06:22,419
objects and that is why allocation is

160
00:06:22,419 --> 00:06:26,350
much faster this collector is generally

161
00:06:26,350 --> 00:06:30,220
best for devices which are very

162
00:06:30,220 --> 00:06:34,419
restrained restricted memory or if you

163
00:06:34,419 --> 00:06:36,190
have your application is being run on a

164
00:06:36,190 --> 00:06:38,800
shared CPU so let's imagine we have a

165
00:06:38,800 --> 00:06:41,800
CPU of say quad-core CPU and we have

166
00:06:41,800 --> 00:06:43,930
four applications applications running

167
00:06:43,930 --> 00:06:47,350
on it if your garbage collector was not

168
00:06:47,350 --> 00:06:48,250
single threaded

169
00:06:48,250 --> 00:06:51,550
and it was multi-threaded then it is

170
00:06:51,550 --> 00:06:53,800
possible that at some point in time your

171
00:06:53,800 --> 00:06:55,510
garbage collector will start all full

172
00:06:55,510 --> 00:06:57,970
threads on four cores of the CPU and

173
00:06:57,970 --> 00:07:00,820
utilize that entire CPU for its own

174
00:07:00,820 --> 00:07:03,010
garbage collection and that is when the

175
00:07:03,010 --> 00:07:05,200
other applications running on the CPU

176
00:07:05,200 --> 00:07:08,620
will suffer so if there are multiple

177
00:07:08,620 --> 00:07:10,930
applications running on a single CPU and

178
00:07:10,930 --> 00:07:13,210
you want to ensure that your garbage

179
00:07:13,210 --> 00:07:15,130
collection does not affect other course

180
00:07:15,130 --> 00:07:18,190
or other applications then you can use

181
00:07:18,190 --> 00:07:21,250
serial collector the next collector to

182
00:07:21,250 --> 00:07:22,600
understand is called the parallel

183
00:07:22,600 --> 00:07:25,570
collector we generally only talk about

184
00:07:25,570 --> 00:07:27,729
parallel old collector which uses

185
00:07:27,729 --> 00:07:30,190
multiple threads for both minor GC as

186
00:07:30,190 --> 00:07:34,570
well as major DC this collector does not

187
00:07:34,570 --> 00:07:36,780
run concurrently with the application

188
00:07:36,780 --> 00:07:37,900
okay

189
00:07:37,900 --> 00:07:39,610
even though its name suggests its paddle

190
00:07:39,610 --> 00:07:42,099
it's named parallel because it has

191
00:07:42,099 --> 00:07:44,020
multiple threads of the garbage

192
00:07:44,020 --> 00:07:45,099
collection it's

193
00:07:45,099 --> 00:07:46,569
and all of those threats are

194
00:07:46,569 --> 00:07:48,969
unparalleled but while the garbage

195
00:07:48,969 --> 00:07:50,620
collection is running all the

196
00:07:50,620 --> 00:07:53,319
application threads are stopped and that

197
00:07:53,319 --> 00:07:55,990
is why if your application is deployed

198
00:07:55,990 --> 00:07:58,779
on a multi-core from multiprocessor

199
00:07:58,779 --> 00:08:01,090
systems then this collector will give

200
00:08:01,090 --> 00:08:03,430
you the greatest throughput that is in

201
00:08:03,430 --> 00:08:04,539
the shortest amount of time

202
00:08:04,539 --> 00:08:06,689
it will be able to collect the highest

203
00:08:06,689 --> 00:08:10,779
amount of garbage possible of course

204
00:08:10,779 --> 00:08:12,249
since it stops the entire application

205
00:08:12,249 --> 00:08:15,370
and it could stop it for some time it is

206
00:08:15,370 --> 00:08:18,490
best only for batch applications so in

207
00:08:18,490 --> 00:08:20,110
the batch application you do not care

208
00:08:20,110 --> 00:08:23,979
about the users the response time

209
00:08:23,979 --> 00:08:26,139
because there is no user on the front

210
00:08:26,139 --> 00:08:28,659
end right it's a batch application it's

211
00:08:28,659 --> 00:08:30,430
running behind the scenes so what you

212
00:08:30,430 --> 00:08:32,289
want is you want the program to run as

213
00:08:32,289 --> 00:08:35,198
efficiently as possible so for batch

214
00:08:35,198 --> 00:08:36,940
applications the best collector to use

215
00:08:36,940 --> 00:08:40,328
is a parallel collector the third

216
00:08:40,328 --> 00:08:44,110
algorithm is called a CMS collector CMS

217
00:08:44,110 --> 00:08:46,420
the full form of CMS is concurrent mark

218
00:08:46,420 --> 00:08:48,970
and sweep and you already saw mark sweep

219
00:08:48,970 --> 00:08:51,880
and compact algorithm earlier and this

220
00:08:51,880 --> 00:08:53,620
is the same thing but it says it's

221
00:08:53,620 --> 00:08:55,990
concurrent moppet mark and sweep that

222
00:08:55,990 --> 00:08:58,360
means it runs concurrently with the

223
00:08:58,360 --> 00:09:00,910
application to mark all the live objects

224
00:09:00,910 --> 00:09:03,490
right so the amount of time that the

225
00:09:03,490 --> 00:09:05,890
application has to stop is less so the

226
00:09:05,890 --> 00:09:10,390
latency of the application is less but

227
00:09:10,390 --> 00:09:12,100
of course during the actual collection

228
00:09:12,100 --> 00:09:15,550
it still has STW pauses STW stop the

229
00:09:15,550 --> 00:09:17,589
world forces that means it stops the

230
00:09:17,589 --> 00:09:19,630
application for a very small amount of

231
00:09:19,630 --> 00:09:23,220
time to do its actual garbage collection

232
00:09:23,220 --> 00:09:26,620
but it's not as bad as the parallel

233
00:09:26,620 --> 00:09:29,620
collector of course there are trade-offs

234
00:09:29,620 --> 00:09:33,130
so it requires more footprint than

235
00:09:33,130 --> 00:09:35,019
parallel collector so it has more data

236
00:09:35,019 --> 00:09:36,790
structures that it needs to take care of

237
00:09:36,790 --> 00:09:39,970
behind the scenes it has less throughput

238
00:09:39,970 --> 00:09:42,339
than the parallel collector but the

239
00:09:42,339 --> 00:09:44,320
advantage of this is it has smaller

240
00:09:44,320 --> 00:09:46,120
pause times than the final collector

241
00:09:46,120 --> 00:09:49,420
right and that is why it is best used

242
00:09:49,420 --> 00:09:52,930
for general applications the improvement

243
00:09:52,930 --> 00:09:55,060
over the CMS collector is called g1

244
00:09:55,060 --> 00:09:58,930
collector g1 collected garbage post

245
00:09:58,930 --> 00:10:01,240
so instead of having and specific young

246
00:10:01,240 --> 00:10:03,339
generation and old generation this

247
00:10:03,339 --> 00:10:05,709
collector uses the entire heap and

248
00:10:05,709 --> 00:10:08,980
divides it into multiple regions and it

249
00:10:08,980 --> 00:10:11,470
itself assigns whether this region is

250
00:10:11,470 --> 00:10:12,820
young generation region or an old

251
00:10:12,820 --> 00:10:15,610
generation region now compared to the

252
00:10:15,610 --> 00:10:17,709
previous collector CMS it has more

253
00:10:17,709 --> 00:10:19,149
footprint so it has even more

254
00:10:19,149 --> 00:10:22,000
requirement for data structures but the

255
00:10:22,000 --> 00:10:24,940
advantage of this is it this has more

256
00:10:24,940 --> 00:10:27,510
predictable latency and this is the best

257
00:10:27,510 --> 00:10:31,360
feature of this connector so when you

258
00:10:31,360 --> 00:10:33,339
start their application you can pass on

259
00:10:33,339 --> 00:10:35,380
this variable that the maximum cost time

260
00:10:35,380 --> 00:10:38,560
that my application can withstand is say

261
00:10:38,560 --> 00:10:41,410
10 milliseconds I cannot handle my

262
00:10:41,410 --> 00:10:43,570
application cannot handle more than 10

263
00:10:43,570 --> 00:10:46,630
milliseconds of cost time of course this

264
00:10:46,630 --> 00:10:48,190
is not a very hard target it's a soft

265
00:10:48,190 --> 00:10:48,610
target

266
00:10:48,610 --> 00:10:50,649
so garbage collector the g1 collector

267
00:10:50,649 --> 00:10:53,649
will try to ensure that the garbage

268
00:10:53,649 --> 00:10:55,959
collection is done only for 10

269
00:10:55,959 --> 00:10:57,940
milliseconds and even if there is some

270
00:10:57,940 --> 00:11:00,100
garbage left then it will take care of

271
00:11:00,100 --> 00:11:02,470
it in the next cycle and will allow the

272
00:11:02,470 --> 00:11:04,269
application to run after 10 milliseconds

273
00:11:04,269 --> 00:11:08,829
and that is why this garbage collector

274
00:11:08,829 --> 00:11:10,630
the chip and collector is best for

275
00:11:10,630 --> 00:11:12,670
applications which need predictable

276
00:11:12,670 --> 00:11:15,310
latency so CMS was great for general

277
00:11:15,310 --> 00:11:18,010
applications it has lower pastimes but

278
00:11:18,010 --> 00:11:19,870
if you want predictable pastimes

279
00:11:19,870 --> 00:11:22,540
predictable latency which you can set

280
00:11:22,540 --> 00:11:25,300
using this variable Max target post and

281
00:11:25,300 --> 00:11:27,329
then you can use the cheaper collector

282
00:11:27,329 --> 00:11:30,100
there is one more collector which is

283
00:11:30,100 --> 00:11:34,120
coming up which is not yet as a default

284
00:11:34,120 --> 00:11:37,839
in JDK is called Shannon Dora Shannon

285
00:11:37,839 --> 00:11:39,760
dower is an improvement upon g1

286
00:11:39,760 --> 00:11:42,430
collector wherein it requires a little

287
00:11:42,430 --> 00:11:44,410
higher footprint so it takes more data

288
00:11:44,410 --> 00:11:47,230
structures behind the scenes but it has

289
00:11:47,230 --> 00:11:50,680
even lower latency than g1 it's going to

290
00:11:50,680 --> 00:11:54,459
come in a few versions of Java so here's

291
00:11:54,459 --> 00:11:56,410
the table complete able to understand

292
00:11:56,410 --> 00:11:59,529
the pros and cons so take a look in

293
00:11:59,529 --> 00:12:01,690
general I would say the serial collector

294
00:12:01,690 --> 00:12:03,760
is for small devices or when you want to

295
00:12:03,760 --> 00:12:05,649
ensure that the GC doesn't affect other

296
00:12:05,649 --> 00:12:08,440
CPUs the parallel collector is best for

297
00:12:08,440 --> 00:12:11,860
batch applications the CMS application

298
00:12:11,860 --> 00:12:13,570
CMS collector is best for general

299
00:12:13,570 --> 00:12:16,390
applications g1 collector is best if you

300
00:12:16,390 --> 00:12:19,120
want predictable latency and in Shanan

301
00:12:19,120 --> 00:12:21,460
Tovah is an improvement upon g1 which

302
00:12:21,460 --> 00:12:23,230
you will be able to use it as default in

303
00:12:23,230 --> 00:12:25,660
few versions of Java that's it for this

304
00:12:25,660 --> 00:12:27,970
video if you have any comments or

305
00:12:27,970 --> 00:12:30,430
questions please let me know and see you

306
00:12:30,430 --> 00:12:33,660
in the next video thank you

